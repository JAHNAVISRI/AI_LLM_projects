{"query": "question: How does the lasso method differ from least squares linear regression? answer: <extra_id_0>", "answers": ["Odds represent the ratio of the probability of an event occurring to it not occurring."], "generation": "The lasso method differs from least squares linear regression because it uses a penalty term to reduce the coefficient estimates.", "passages": [{"id": 9, "contributed_by": "group 1", "title": "", "section": "", "text": "The lasso, discussed in Chapter 6, relies upon a linear model but uses an alternative fitting procedure for estimating the coefficients β0, β1, . . . , βp. The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors—namely, those with nonzero coefficient estimates."}, {"id": 899, "contributed_by": "group 10", "title": "", "section": "", "text": "In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefcient towards zero by a constant amount,"}, {"id": 298, "contributed_by": "group 3", "title": "", "section": "", "text": "Although it is possible that if we were to spend more time, and got the form and amount of regularization just right, that we might be able to match or even outperform linear regression and the lasso."}, {"id": 543, "contributed_by": "group 6", "title": "", "section": "", "text": "The lasso, when applied to a linear model, has specific expectations regarding the coefficients. It assumes that most of the coefficients are exactly zero. This key characteristic sets the lasso apart from other regularization techniques. Unlike the ridge regression, which expects coefficients to be small but doesn't enforce them to be exactly zero, the lasso enforces a sparsity constraint, making it a useful tool for feature selection. The lasso operates under the assumption that many features are irrelevant or redundant for predicting the target variable. By pushing a substantial number of coefficients to zero, it effectively selects a subset of the most important features, simplifying the model. This makes the lasso a valuable method for preventing overfitting and improving the interpretability of linear models, particularly when dealing with high-dimensional datasets."}, {"id": 898, "contributed_by": "group 10", "title": "", "section": "", "text": "Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret."}, {"id": 157, "contributed_by": "group 2", "title": "", "section": "", "text": "These two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set."}, {"id": 896, "contributed_by": "group 10", "title": "", "section": "", "text": "When lambda = 0, then the lasso simply gives the least squares fit, and when lambda becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero."}, {"id": 895, "contributed_by": "group 10", "title": "", "section": "", "text": "When lambda = 0, then the lasso simply gives the least squares fit, and when lambda becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero."}, {"id": 164, "contributed_by": "group 2", "title": "", "section": "", "text": "We can see that ridge regression and the lasso perform two very different types of shrinkage. In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, lambda divided by 2; the least squares coefficients that are less than lambda divided by 2 in absolute value are shrunken entirely to zero. The type of shrinkage performed by the lasso in this simple setting is known as soft-thresholding. The fact that some lasso coefficients are shrunken entirely to zero explains why the lasso performs feature selection."}, {"id": 156, "contributed_by": "group 2", "title": "", "section": "", "text": "As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the l1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter lambda is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models that is, models that involve only a subset of the variables."}, {"id": 894, "contributed_by": "group 10", "title": "", "section": "", "text": "As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression."}, {"id": 887, "contributed_by": "group 10", "title": "", "section": "", "text": "It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can signifcantly reduce their variance.The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso."}, {"id": 159, "contributed_by": "group 2", "title": "", "section": "", "text": "We note that even though PCR provides a simple way to perform regression using M less than p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features. Therefore, while PCR often performs quite well in many practical settings, it does not result in the development of a model that relies upon a small set of the original features. In this sense, PCR is more closely related to ridge regression than to the lasso. In fact, one can show that PCR and ridge regression are very closely related. One can even think of ridge regression as a continuous version of PCR."}, {"id": 559, "contributed_by": "group 6", "title": "", "section": "", "text": "Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables, X, X2, and X3, as predictors. This approach provides a simple way to provide a nonlinear fit to data."}, {"id": 546, "contributed_by": "group 6", "title": "", "section": "", "text": "The lasso, a critical property is demonstrated when the cross-validation error corresponds to a set of coefficient estimates with only some variables being non-zero. This property is known as Feature selection. The Lasso (Least Absolute Shrinkage and Selection Operator) is a linear regression technique that introduces regularization by adding a penalty term to the linear regression cost function. This penalty term encourages sparsity in the coefficient estimates, which means that it promotes a situation where some of the coefficients are exactly zero. The goal of the Lasso is to select a subset of the most important features while reducing the impact of less relevant features. When the cross-validation error leads to a set of non-zero coefficients for only some variables, it indicates that the Lasso has successfully identified and selected the most significant features, effectively performing feature selection. This is crucial for avoiding overfitting, which occurs when the model is too complex and fits the noise in the data, as well as for addressing multicollinearity, where highly correlated predictors can cause problems in regression."}, {"id": 558, "contributed_by": "group 6", "title": "", "section": "", "text": "In Chapter 6 we see that we can improve upon least squares using ridge regression, the lasso, principal components regression, and other techniques. In that setting, the improvement is obtained by reducing the complexity of the linear model, and hence the variance of the estimates."}, {"id": 542, "contributed_by": "group 6", "title": "", "section": "", "text": "The key difference between the ridge regression penalty and the lasso penalty lies in their regularization techniques. Ridge regression, represented by option A, employs an L2 penalty, which adds the sum of squared coefficients to the cost function. This results in all coefficient values being shrunk towards zero, although none are exactly zero. On the other hand, the lasso penalty, as described in option B, utilizes an L1 penalty. In the lasso, some coefficients can be precisely shrunk to zero, effectively performing feature selection."}, {"id": 166, "contributed_by": "group 2", "title": "", "section": "", "text": "The Gaussian and double-exponential priors are displayed in Figure. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for beta. Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero."}, {"id": 540, "contributed_by": "group 6", "title": "", "section": "", "text": "The lasso, a popular technique in regression modeling, aims to address the issue of overfitting in predictive models. Overfitting occurs when a model includes too many predictors or features, leading to excessive complexity. This can result in a model that performs exceptionally well on the training data but poorly on new, unseen data. The lasso mitigates this problem by introducing a penalty term that encourages the model to set some of the coefficients of predictors to zero, effectively selecting a subset of the most relevant features. This process helps prevent overfitting and results in a more parsimonious model that is better at generalizing to new data."}, {"id": 429, "contributed_by": "group 5", "title": "Extensions of the Linear Model: 3.3.2", "section": "3.3.2", "text": "The additivity assumption means that the association between a predictor Xj and the response Y does not depend on the values of the other predictors."}, {"id": 420, "contributed_by": "group 5", "title": "Simple Linear Regression: 3.1", "section": "3.1", "text": "Simple linear regression lives up to its name: it is a very straightforward approach for predicting a quantitative response Y on the basis of a single predictor variable X."}, {"id": 165, "contributed_by": "group 2", "title": "", "section": "", "text": "If g is a Gaussian distribution with mean zero and standard deviation a function of lambda, then it follows that the posterior mode for beta—that is, the most likely value for beta, given the data—is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.). If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of lambda, then it follows that the posterior mode for beta is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.) The Gaussian and double-exponential priors are displayed in Figure. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for beta. Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero."}, {"id": 512, "contributed_by": "group 6", "title": "", "section": "", "text": "_ in range(B): idx = rng. choice(D.index , n, replace =True) value = func(D, idx) first_ += value second_ += value **2 return np.sqrt( second_ / B - ( first_ / B)**2) Notice the use of as a loop variable in for _ in range(B). This is o ten used i the value of the counter is unimportant and simply makes sure the loop is executed B times. Let s use our unction to evaluate the accuracy of our estimate o   using B = 1,000 bootstrap replications. In [19]: Out[19]: alpha_SE = boot_SE (alpha_func , Portfolio , B=1000, seed =0) alpha_SE 0.0912 The nal output shows that the bootstrap estimate or SE(  ) is 0.0912. Estimating the Accuracy of a Linear Regression Model The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the 222 5. Resampling Methods In [20]: estimates or  0 and  1, the intercept and slope terms or the linear regres-sion model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas or SE( 0) and SE( 1) described in Section 3.1.2.     To use our boot_SE() unction, we must write a unction (its first argument) that takes a data frame D and indices idx as its only arguments. But here we want to bootstrap a specific regression model, specified by a model formula"}, {"id": 823, "contributed_by": "group 9", "title": "", "section": "", "text": "In the multiple regression setting with p predictors, we need to ask whether all of the regression coefficients are zero. As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis,"}, {"id": 521, "contributed_by": "group 6", "title": "", "section": "", "text": "following our models using least squares: i. Y = 0+ 1X+ ii. Y = 0+ 1X+ 2X2+ iii. Y = 0+ 1X+ 2X2+ 3X3+ iv. Y = 0+ 1X+ 2X2+ 3X3+ 4X4. Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y . (d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why? (e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer. ( ) Comment on the statistical significance of the coefficient estimates that results from fitting each o the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results? 9. We will now consider the Boston housing data set, from the ISLP library. (a) Based on this data set, provide an estimate or the population mean o medv. Call this estimate  . 228 5. Resampling Methods (b) Provide an estimate of the standard error o  . Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root or the number of observations. (c) Now estimate the standard error o   using the bootstrap. How does this compare to your answer from (b)? (d) Based on your bootstrap estimate rom (c), provide a 95 % confidence interval or the mean o medv. Compare it to the results obtained by using Boston['medv'].std() and the two standard error rule (3.9). Hint: You can approximate"}, {"id": 169, "contributed_by": "group 2", "title": "", "section": "", "text": "Polynomial regression extends the linear model by including additional predictor variables, obtained by raising the original predictors to various powers. For example, a cubic regression uses three variables, X, X squared, and X cube, as predictors. This approach provides a simple way to provide a nonlinear fit to data."}, {"id": 76, "contributed_by": "group 1", "title": "", "section": "", "text": "In Chapter 3, we used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the model, the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood."}, {"id": 815, "contributed_by": "group 9", "title": "", "section": "", "text": "The model given by (3.5) defines the population regression line, which is the best linear approximation to the true relationship between X and Y."}, {"id": 177, "contributed_by": "group 2", "title": "", "section": "", "text": "The weights Ki0 will differ for each value of x0. In other words, in order to obtain the local regression fit at a new point, we need to fit a new weighted least squares regression model by minimizing (7.14) for a new set of weights."}, {"id": 442, "contributed_by": "group 5", "title": "Some Important Questions: 3.2.2", "section": "3.2.2", "text": "The new (p − 1)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached."}, {"id": 510, "contributed_by": "group 6", "title": "", "section": "", "text": "results ['test_score '] .outer() np.power() KFold() Shuffle Split() 220 5. Resampling Methods Out[13]: array ([23.6166]) One can estimate the variability in the test error by running the ollowing: In [14]: Out[14]: validation = ShuffleSplit ( n_splits =10, test_size =196 , random_state =0) results = cross_validate (hp_model , Auto.drop (['mpg '], axis =1) , Auto['mpg '], cv= validation ) results ['test_score ']. mean (), results ['test_score ']. std () (23.8022, 1.4218) Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Monte Carlo variation incurred by picking different random olds. In [15]: 5.3.3 The Bootstrap We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set. Estimating the Accuracy of a Statistic o Interest One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. While there are several implementations of the bootstrap in Python, its use or estimating standard error is simple enough that we write our own function below or the case when our data is stored in a data frame. To illustrate the bootstrap, we start with a simple example. The Portfolio data set in the ISLP package is described in Section 5.2. The goal is"}, {"id": 541, "contributed_by": "group 6", "title": "", "section": "", "text": "Ridge regression, the behavior of coefficient estimates is influenced by the tuning parameter λ. As λ increases, the coefficient estimates undergo a specific transformation. Option A, which suggests that the coefficient estimates remain the same, is not accurate in this context. The correct answer is Option C, which states that the coefficient estimates decrease in magnitude. This decrease in magnitude is a fundamental characteristic of ridge regression. Ridge regression is a regularization technique used to mitigate multicollinearity and overfitting in linear regression. It does this by adding a penalty term to the ordinary least squares (OLS) objective function. The penalty term is proportional to the L2 norm of the coefficient vector, and λ controls the strength of this penalty. As λ increases, the impact of the penalty term on the coefficient estimates becomes more pronounced, leading to a decrease in the absolute values of the coefficients. This means that, in ridge regression, the coefficients tend to be smaller as λ increases, effectively shrinking them towards zero."}, {"id": 446, "contributed_by": "group 5", "title": "Some Important Questions: 3.2.2", "section": "3.2.2", "text": "We begin with the null model, which contains an intercept but no predictors. We then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS."}, {"id": 168, "contributed_by": "group 2", "title": "", "section": "", "text": "However, standard linear regression can have significant limitations in terms of predictive power. This is because the linearity assumption is almost always an approximation, and sometimes a poor one."}, {"id": 430, "contributed_by": "group 5", "title": "Extensions of the Linear Model: 3.3.2", "section": "3.3.2", "text": "The linearity assumption states that the change in the response Y associated with a one-unit change in Xj is constant, regardless of the value of Xj."}, {"id": 831, "contributed_by": "group 9", "title": "", "section": "", "text": "If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as log X, √X, and X2, in the regression model."}, {"id": 505, "contributed_by": "group 6", "title": "", "section": "", "text": "= sm.OLS(y_train , X_train ) results = model.fit () We now use the predict() method of results evaluated on the model ma-trix or this model created using the validation data set. We also calculate the validation MSE of our model. In [5]: X_valid = hp_mm. transform ( Auto_valid ) y_valid = Auto_valid ['mpg '] valid_pred = results . predict ( X_valid ) np.mean (( y_valid - valid_pred )**2) Out[5]: 23.6166 Hence our estimate or the validation MSE of the linear regression t is 23.62. We can also estimate the validation error or higher-degree polynomial regressions. We rst provide a unction evalMSE() that takes a model string as well as a training and test set and returns the MSE on the test set. In [6]: def evalMSE (terms , response , train , test): mm = MS(terms) X_train = mm. fit_transform (train) y_train = train[ response ] X_test = mm. transform (test) y_test = test[ response ] train_test_ split() 5.3 Lab: Cross-Validation and the Bootstrap 217 results = sm.OLS(y_train , X_train ).fit () test_pred = results . predict ( X_test) return np.mean (( y_test - test_pred )**2) In [7]: Out[7]: Let s use this function to estimate the validation MSE using linear, quadratic and cubic ts. We use the enumerate() unction here, which gives enumerate() both the values and indices of objects as one iterates over a or loop. MSE = np.zeros (3) for idx , degree in enumerate (range (1, 4)): MSE[idx] = evalMSE ([ poly('horsepower ', degree )], 'mpg ', Auto_train , Auto_valid ) MSE array"}, {"id": 892, "contributed_by": "group 10", "title": "", "section": "", "text": "linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates."}, {"id": 705, "contributed_by": "group 8", "title": "Shrinkage for the Cox Model: 11.6", "section": "11.6", "text": "We apply the lasso-penalized Cox model to the Publication data. We first randomly split the 244 trials into equally sized training and test sets. From the cross-validation results from the training set, the partial likelihood deviance, shown on the y-axis, is twice the cross-validated negative log partial likelihood, it plays the role of the cross-validation error. Note the U-shape of the partial likelihood deviance. The cross-validation error is minimized for an intermediate level of model complexity. Specifically, this occurs when just two predictors, budget and impact, have non-zero estimated coefficients."}, {"id": 436, "contributed_by": "group 5", "title": "Comparison of Linear Regression with K-Nearest Neighbors: 3.5", "section": "3.5", "text": "Here we consider one of the simplest and best-known non-parametric methods, K-nearest neighbors regression (KNN regression)."}], "metadata": {}, "id": 9}
{"query": "question: Why can highly flexible methods lead to overfitting? answer: <extra_id_0>", "answers": ["Highly flexible methods can lead to overfitting because they might adapt too closely to the training data, including its noise or errors, rather than capturing the underlying relationship."], "generation": "Highly flexible methods lead to overfitting because they are too flexible to fit the training data.", "passages": [{"id": 10, "contributed_by": "group 1", "title": "", "section": "", "text": "We have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accurately—interpretability is not a concern. In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods."}, {"id": 783, "contributed_by": "group 9", "title": "", "section": "", "text": "We can try to address this problem by choosing flexible models that can fit many different possible functional forms flexible for f. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they overfitting follow the errors, or noise, too closely."}, {"id": 6, "contributed_by": "group 1", "title": "", "section": "", "text": "One might reasonably ask the following question: why would we ever choose to use a more restrictive method instead of a very flexible approach? There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between Y and X1, X2, . . . , Xp. In contrast, very flexible approaches, such as the splines discussed in Chapter 7 and displayed in Figures 2.5 and 2.6, and the boosting methods discussed in Chapter 8, can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response."}, {"id": 271, "contributed_by": "group 3", "title": "", "section": "", "text": "Overfitting can be controlled via the various forms of regularization."}, {"id": 24, "contributed_by": "group 1", "title": "", "section": "", "text": "What do we mean by the variance and bias of a statistical learning method? Variance refers to the amount by which ˆf would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different ˆf. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in ˆf. In general, more flexible statistical methods have higher variance."}, {"id": 394, "contributed_by": "group 5", "title": "What is Statistical Learning: The Trade-Off Between Prediction Accuracy and Model Interpretability", "section": "The Trade-Off Between Prediction Accuracy and Model Interpretability", "text": "For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between Y and X1,X2, . . . ,Xp. In contrast, very flexible approaches, such as the splines discussed and the boosting methods, can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response."}, {"id": 529, "contributed_by": "group 6", "title": "", "section": "", "text": "When the number of observations (n) is not much larger than the number of predictor variables (p) in linear regression modeling using least squares, it results in overfitting, leading to poor predictions on future observations. This is because the model lacks sufficient data to generalize well and instead becomes overly sensitive to noise in the training data. This situation can lead to a situation where the model fits the training data very closely but fails to generalize to new, unseen data, causing a decrease in predictive performance. In such cases, the model is more likely to have high variance, and it may struggle to make accurate predictions beyond the training dataset. This condition can be particularly problematic when the number of predictor variables is close to or exceeds the number of observations. Overfitting can be mitigated through techniques such as regularization or by increasing the size of the training dataset."}, {"id": 223, "contributed_by": "group 3", "title": "", "section": "", "text": "An ensemble method is an approach that combines many simple “building block” models in order to obtain a single and potentially very powerful model. These simple building block models are sometimes known as weak learners, since they may lead to mediocre predictions on their own."}, {"id": 607, "contributed_by": "group 7", "title": "", "section": "", "text": "An ensemble method is an approach that combines many simple “building block” models in order to obtain a single and potentially very powerful model. These simple building block models are sometimes known as weak learners, since they may lead to mediocre predictions on their own."}, {"id": 535, "contributed_by": "group 6", "title": "", "section": "", "text": "When dealing with a situation where the number of predictor variables (p) substantially exceeds the number of observations (n), one of the most appropriate methods to consider is Cross-validation. Cross-validation is particularly valuable in this scenario as it helps assess the model's performance by splitting the available data into subsets, often using techniques like k-fold cross-validation. This approach enables us to mitigate issues related to overfitting and evaluate the model's generalization capabilities. The other options, such as Best subset selection, Forward stepwise selection, and Backward stepwise selection, are also methods used for variable selection and model building, but they may not be as suitable when dealing with a high-dimensional dataset where p >> n. Therefore, in such cases, Cross-validation is a valuable technique to ensure robust model performance."}, {"id": 142, "contributed_by": "group 2", "title": "", "section": "", "text": "For computational reasons, best subset selection cannot be applied with very large p. Best subset selection may also suffer from statistical problems when p is large. The larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection."}, {"id": 400, "contributed_by": "group 5", "title": "Assessing Model Accuracy: Measuring the Quality of Fit", "section": "Measuring the Quality of Fit", "text": "When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f."}, {"id": 8, "contributed_by": "group 1", "title": "", "section": "", "text": "When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE."}, {"id": 551, "contributed_by": "group 6", "title": "", "section": "", "text": "A high-dimensional data set is characterized by a number of features (p) that exceeds the number of observations (n). This is represented as p > n. In such a scenario, the data is said to be sparse, meaning that there are more features than data points to support them. This can lead to challenges in data analysis and modeling, as traditional methods may not be well-suited for handling such high-dimensionality."}, {"id": 562, "contributed_by": "group 6", "title": "", "section": "", "text": "Generalized additive models allow us to extend the methods above to deal with multiple predictors. In Sections 7.1–7.6, we present a number of approaches for modeling the relationship between a response Y and a single predictor X in a flexible way. In Section 7.7, we show that these approaches can be seamlessly integrated in order to model a response Y as a function of several predictors X1,...,Xp."}, {"id": 632, "contributed_by": "group 7", "title": "", "section": "", "text": "In practice, C is treated as a tuning parameter that is generally chosen via cross-validation. C controls the bias-variance trade-off of the statistical learning technique. When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance."}, {"id": 987, "contributed_by": "group 11", "title": "", "section": "", "text": "Tree Pruning The process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex"}, {"id": 146, "contributed_by": "group 2", "title": "", "section": "", "text": "In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:  1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.  2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.  We consider both of these approaches below."}, {"id": 445, "contributed_by": "group 5", "title": "Some Important Questions: 3.2.2", "section": "3.2.2", "text": "Forward selection is a greedy approach and might include variables early that later become redundant. Mixed selection can remedy this."}, {"id": 487, "contributed_by": "group 6", "title": "", "section": "", "text": "5 Resampling Methods Resampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and retting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample. Resampling approaches can be computationally expensive, because they involve fitting the same statistical method multiple times using different subsets of the training data. However, due to recent advances in computing power, the computational requirements of resampling methods generally are not prohibitive. In this chapter, we discuss two of the most commonly used resampling methods, cross-validation and the bootstrap. Both methods are important tools in the practical application of many statistical learning procedures. For example, cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model s performance is known as model assessment, whereas the process of selecting the proper level of flexibility or a model is known as model selection. The bootstrap is used in several context"}, {"id": 227, "contributed_by": "group 3", "title": "", "section": "", "text": "We have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here. Boosting has three tuning parameters: 1. The number of trees B. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B."}, {"id": 553, "contributed_by": "group 6", "title": "", "section": "", "text": "Traditional approaches like Cp, AIC, and BIC are not appropriate in the high-dimensional setting when adjusting for the number of variables because estimating sigma square, the variance of the error term, is problematic in high dimensions. In high dimensions, the number of parameters to be estimated is often much larger than the number of observations. This can lead to overfitting, where the model is too complex and fits the noise in the data rather than the underlying patterns. As a result, the estimated value of sigma square can be biased downwards. This can lead to the underestimation of the model complexity, which can result in the selection of too many variables."}, {"id": 510, "contributed_by": "group 6", "title": "", "section": "", "text": "results ['test_score '] .outer() np.power() KFold() Shuffle Split() 220 5. Resampling Methods Out[13]: array ([23.6166]) One can estimate the variability in the test error by running the ollowing: In [14]: Out[14]: validation = ShuffleSplit ( n_splits =10, test_size =196 , random_state =0) results = cross_validate (hp_model , Auto.drop (['mpg '], axis =1) , Auto['mpg '], cv= validation ) results ['test_score ']. mean (), results ['test_score ']. std () (23.8022, 1.4218) Note that this standard deviation is not a valid estimate of the sampling variability of the mean test score or the individual scores, since the randomly-selected training samples overlap and hence introduce correlations. But it does give an idea of the Monte Carlo variation incurred by picking different random olds. In [15]: 5.3.3 The Bootstrap We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set. Estimating the Accuracy of a Statistic o Interest One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. While there are several implementations of the bootstrap in Python, its use or estimating standard error is simple enough that we write our own function below or the case when our data is stored in a data frame. To illustrate the bootstrap, we start with a simple example. The Portfolio data set in the ISLP package is described in Section 5.2. The goal is"}, {"id": 406, "contributed_by": "group 5", "title": "Assessing Model Accuracy: The Bias-Variance Trade-Off", "section": "The Bias-Variance Trade-Off", "text": "The relative rate of change of bias and variance determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases."}, {"id": 512, "contributed_by": "group 6", "title": "", "section": "", "text": "_ in range(B): idx = rng. choice(D.index , n, replace =True) value = func(D, idx) first_ += value second_ += value **2 return np.sqrt( second_ / B - ( first_ / B)**2) Notice the use of as a loop variable in for _ in range(B). This is o ten used i the value of the counter is unimportant and simply makes sure the loop is executed B times. Let s use our unction to evaluate the accuracy of our estimate o   using B = 1,000 bootstrap replications. In [19]: Out[19]: alpha_SE = boot_SE (alpha_func , Portfolio , B=1000, seed =0) alpha_SE 0.0912 The nal output shows that the bootstrap estimate or SE(  ) is 0.0912. Estimating the Accuracy of a Linear Regression Model The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the 222 5. Resampling Methods In [20]: estimates or  0 and  1, the intercept and slope terms or the linear regres-sion model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas or SE( 0) and SE( 1) described in Section 3.1.2.     To use our boot_SE() unction, we must write a unction (its first argument) that takes a data frame D and indices idx as its only arguments. But here we want to bootstrap a specific regression model, specified by a model formula"}, {"id": 118, "contributed_by": "group 2", "title": "", "section": "", "text": "But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not."}, {"id": 162, "contributed_by": "group 2", "title": "", "section": "", "text": "The third point above is in fact a key principle in the analysis of high-dimensional data, which is known as the curse of dimensionality. One might think that as the number of features used to fit a model increases, the quality of the fitted model will increase as well. However, comparing the left-hand and right-hand panels in Figure, we see that this is not necessarily the case: in this example, the test set MSE almost doubles as p increases from 20 to 2,000. In general, adding additional signal features that are truly associated with the response will improve the fitted model, in the sense of leading to a reduction in test set error. However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model, and consequently an increased test set error. This is because noise features increase the dimensionality of the problem, exacerbating the risk of overfitting (since noise features may be assigned nonzero coefficients due to chance associations with the response on the training set) without any potential upside in terms of improved test set error. Thus, we see that new technologies that allow for the collection of measurements for thousands or millions of features are a double-edged sword: they can lead to improved predictive models if these features are in fact relevant to the problem at hand, but will lead to worse results if the features are not relevant. "}, {"id": 941, "contributed_by": "group 10", "title": "", "section": "", "text": "Python package pygam can be used to fit GAMs using smoothing splines, via an approach known as backfitting. This method fits a model involving multiple predictors by repeatedly updating the fit for each predictor in turn, holding the others fixed."}, {"id": 517, "contributed_by": "group 6", "title": "", "section": "", "text": "We will now investigate numerically the probability that a boot-strap sample o size n = 100 contains the jth observation. Here j = 4. We first create an array store with values that will subsequently be overwritten using the unction np.empty(). We then np.empty() 5.4 Exercises 225 repeatedly create bootstrap samples, and each time we record whether or not the th observation is contained in the bootstrap sample. rng = np. random. default_rng (10) store = np.empty (10000) for i in range (10000) : store[i] = np.sum(rng.choice (100 , replace =True) == 4) > 0 np.mean(store) Comment on the results obtained. 3. We now review k- old cross-validation. (a) Explain how k- old cross-validation is implemented. (b) What are the advantages and disadvantages o k- old cross-validation relative to: i. The validation set approach? ii. LOOCV? 4. Suppose that we use some statistical learning method to make a pre-diction or the response Y or a particular value of the predictor X. Carefully describe how we might estimate the standard deviation o our prediction. Applied 5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed be ore beginning your analysis. (a) Fit a logistic regression model that uses income and balance to predict default. (b) Using the validation set approach, estimate the test error of this model. In order"}, {"id": 595, "contributed_by": "group 7", "title": "", "section": "", "text": "Boosting has three tuning parameters: The number of trees B, the shrinkage parameter λ, and the number d of splits in each tree. Unlike bagging and random forests, boosting can overfit if B is too large."}, {"id": 186, "contributed_by": "group 2", "title": "", "section": "", "text": "Although a smoothing spline has n parameters and hence n nominal degrees of freedom, these n parameters are heavily constrained or shrunk down. Hence measure of the flexibility of the smoothing spline—the higher it is, the more flexible"}, {"id": 283, "contributed_by": "group 3", "title": "", "section": "", "text": "Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that perform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches."}, {"id": 552, "contributed_by": "group 6", "title": "", "section": "", "text": "In regression analysis, when the number of features (p) is as large as, or larger than, the number of observations (n), the least squares method yields a perfect fit to the data. However, this often leads to overfitting, where the model captures the noise in the training data rather than the underlying relationship between the features and the target variable. As a result, the model may not perform well on an independent test set. In other words, when there are more features than observations, the least squares method is able to find a solution that perfectly fits the training data, even if the relationship between the features and the target variable is not actually linear. This can lead to the model picking up on random noise in the data, which can then be used to make inaccurate predictions on new data."}, {"id": 442, "contributed_by": "group 5", "title": "Some Important Questions: 3.2.2", "section": "3.2.2", "text": "The new (p − 1)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached."}, {"id": 937, "contributed_by": "group 10", "title": "", "section": "", "text": "Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 using only the nearby training observations."}, {"id": 120, "contributed_by": "group 2", "title": "", "section": "", "text": "It was mentioned in Section 5.1.1 that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set."}, {"id": 540, "contributed_by": "group 6", "title": "", "section": "", "text": "The lasso, a popular technique in regression modeling, aims to address the issue of overfitting in predictive models. Overfitting occurs when a model includes too many predictors or features, leading to excessive complexity. This can result in a model that performs exceptionally well on the training data but poorly on new, unseen data. The lasso mitigates this problem by introducing a penalty term that encourages the model to set some of the coefficients of predictors to zero, effectively selecting a subset of the most relevant features. This process helps prevent overfitting and results in a more parsimonious model that is better at generalizing to new data."}, {"id": 176, "contributed_by": "group 2", "title": "", "section": "", "text": "Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 regression using only the nearby training observations."}, {"id": 953, "contributed_by": "group 10", "title": "", "section": "", "text": "The span plays a role like that of the tuning parameter in smoothing splines it controls the flexibility of the non-linear fit. The smaller the value of s, the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations."}, {"id": 103, "contributed_by": "group 2", "title": "", "section": "", "text": "In this chapter, we discuss two of the most commonly used resampling methods, cross-validation and the bootstrap. Both methods are important tools in the practical application of many statistical learning procedures."}], "metadata": {}, "id": 10}
