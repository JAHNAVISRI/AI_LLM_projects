{"id": 61, "contributed_by": "group 1", "question": "What is the impact of correlated error terms on linear regression?", "answers": ["Correlated error terms can lead to underestimated standard errors, narrower confidence intervals, and lower p-values, which may result in an unwarranted sense of confidence in the model."]}
{"id": 62, "contributed_by": "group 1", "question": "How can you determine if error terms are correlated in time series data?", "answers": ["To determine if error terms are correlated in time series data, you can plot the residuals as a function of time. Correlated error terms may result in tracking in the residuals, where adjacent residuals have similar values."]}
{"id": 63, "contributed_by": "group 1", "question": "How does collinearity affect linear regression modeling?", "answers": ["Collinearity occurs when two or more predictor variables are closely related, making it difficult to separate their individual effects on the response. Collinearity can lead to unstable coefficient estimates and reduced accuracy."]}
{"id": 64, "contributed_by": "group 1", "question": "What's the deal with a 'parametric approach' in regression, and why might it be better than something like K-nearest neighbors?", "answers": ["A 'parametric approach in regression is like assuming a specific rule for how things are connected, like a straight line in linear regression. It's cool because it's simpler and only needs a few numbers to work out, making it easier to understand and use tests to check if it's accurate."]}
{"id": 65, "contributed_by": "group 1", "question": "n K-nearest neighbors (KNN) regression, what does the 'K' value do, and how does it change the smoothness of the prediction line?", "answers": ["The 'K' value in KNN determines how many nearby data points are used to predict the outcome. A smaller 'K' makes the prediction line less smooth, as it closely follows individual data points, while a larger 'K' results in a smoother prediction line by averaging more data points."]}
{"id": 66, "contributed_by": "group 1", "question": "What's the curse of dimensionality, and how does it affect K-nearest neighbors? ", "answers": ["The curse of dimensionality refers to the challenges of dealing with high-dimensional data, where distances between points become less meaningful. It affects KNN because in high dimensions, there are few nearby data points, making KNN less reliable. "]}
{"id": 67, "contributed_by": "group 1", "question": "When would we go for linear regression instead of K-nearest neighbors, even if KNN is a bit better at predicting?", "answers": ["We'd choose linear regression over K-nearest neighbors when we prefer a simple model, even if KNN predicts slightly better. Linear regression gives a straightforward model with a few numbers, making it easier to understand and explain, and it can use statistical tests."]}
{"id": 68, "contributed_by": "group 1", "question": "What is the relationship between classification and regression methods?", "answers": ["Classification predicts qualitative responses, while regression predicts quantitative ones."]}
{"id": 69, "contributed_by": "group 1", "question": "What are the assumptions made by LDA and naive Bayes?", "answers": ["LDA assumes features are normally distributed with a common within-class covariance matrix, while naive Bayes assumes feature independence."]}
{"id": 70, "contributed_by": "group 1", "question": "What is the Bayes classifier's approach?", "answers": ["The Bayes classifier assigns an observation to the class with the greatest posterior probability."]}
{"id": 71, "contributed_by": "group 1", "question": "Why might one lower the threshold for the posterior probability in the Bayes classifier?", "answers": ["To address concerns about incorrectly predicting certain statuses, like defaulting, one might lower the threshold to be more cautious."]}
{"id": 72, "contributed_by": "group 1", "question": "What is the role of the logistic function in logistic regression?", "answers": ["The logistic function ensures that the predicted probabilities are between 0 and 1."]}
{"id": 73, "contributed_by": "group 1", "question": "How does logistic regression relate to linear regression for binary responses?", "answers": ["Logistic regression is preferable for binary responses as linear regression can predict probabilities outside the [0, 1] interval."]}
{"id": 74, "contributed_by": "group 1", "question": "What is the nature of the decision boundary in LDA?", "answers": ["In LDA, the decision boundary is linear."]}
{"id": 75, "contributed_by": "group 1", "question": "What are the odds in the context of logistic regression?", "answers": ["Odds represent the ratio of the probability of an event occurring to it not occurring."]}
{"id": 76, "contributed_by": "group 1", "question": "How are the coefficients in logistic regression estimated?", "answers": ["They are estimated using the method of maximum likelihood."]}
{"id": 77, "contributed_by": "group 1", "question": "Why is logistic regression preferred over linear regression for classification?", "answers": ["Linear regression can't handle qualitative responses with more than two classes and might not provide meaningful probability estimates."]}
{"id": 78, "contributed_by": "group 1", "question": "Why might one use QDA over LDA?", "answers": ["QDA can be used when the decision boundaries are moderately non-linear, making it more flexible than LDA."]}
{"id": 79, "contributed_by": "group 1", "question": "How does the relationship between p(X) and X in logistic regression differ from a straight-line relationship?", "answers": ["The relationship between p(X) and X in logistic regression is not a straight line. The rate of change in p(X) per unit change in X depends on the current value of X."]}
{"id": 80, "contributed_by": "group 1", "question": "What is the Bayes\u2019 theorem?", "answers": ["Bayes' theorem calculates the probability of an event based on prior knowledge of conditions related to the event. It relates current evidence to prior beliefs in a systematic way."]}
