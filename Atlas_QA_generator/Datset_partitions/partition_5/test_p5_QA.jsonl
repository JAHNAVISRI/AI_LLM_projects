{"id": 161, "contributed_by": "group 2", "question": "What happens when traditional statistical techniques like ordinary least squares (OLS) regression are applied to high-dimensional data?", "answers": ["Ordinary least squares (OLS) regression may result in a perfect fit to the training data, but it often performs poorly on new unseen data leading to overfitting."]}
{"id": 162, "contributed_by": "group 2", "question": "How does the curse of dimensionality relate to high-dimensional data analysis?", "answers": ["The curse of dimensionality refers to the challenge of dealing with high-dimensional data where increasing the number of features, especially irrelevant ones, can lead to a significant increase in the risk of overfitting, potentially degrading the model's performance."]}
{"id": 163, "contributed_by": "group 2", "question": "In the context of high-dimensional data analysis, why is it difficult to identify the best variables and coefficients for a regression model?", "answers": ["In high dimensions, multicollinearity is extreme, making it challenging to determine which variables are truly predictive. Any variable in the model can be expressed as a linear combination of all other variables, making it difficult to pinpoint the best coefficients or predictors."]}
{"id": 164, "contributed_by": "group 2", "question": "How does lasso perform feature selection?", "answers": ["The lasso shrinks each least squares coefficient towards zero by a constant amount i.e. by lambda divided by 2. The least squares coefficients that are less than lambda divided by 2 in absolute value are shrunken entirely to zero. The fact that some lasso coefficients are shrunken entirely to zero explains how the lasso performs feature selection."]}
{"id": 165, "contributed_by": "group 2", "question": "How do ridge regression and the lasso relate to a Bayesian viewpoint of regression?", "answers": ["Ridge regression and the lasso can be viewed through a Bayesian lens. Ridge regression corresponds to a Gaussian prior on the coefficient vector, while the lasso corresponds to a double-exponential (Laplace) prior. In Bayesian terms, the solutions provided by ridge regression and the lasso are the posterior modes of the coefficient vector, reflecting the most likely values for the coefficient vector beta given the data."]}
{"id": 166, "contributed_by": "group 2", "question": "What does it mean when the lasso prior is described as steeply peaked at zero?", "answers": ["The lasso prior having a peak at zero indicates that it expects many coefficients to be exactly zero."]}
{"id": 167, "contributed_by": "group 2", "question": "What is the computational advantage of ridge regression over best subset selection?", "answers": ["Even for moderate values for p, the search can be computationally infeasible in best subset selection. In contrast, for any fixed value of lambda, ridge regression only fits a single model, and the model fitting procedure can be performed quite quickly."]}
{"id": 168, "contributed_by": "group 2", "question": "What are the main limitations of standard linear regression?", "answers": ["The main limitations of standard linear regression include its assumption of linearity, which can be a poor approximation in many real-world scenarios, and its inability to capture complex non-linear relationships between predictors and the response variable."]}
{"id": 169, "contributed_by": "group 2", "question": "How does polynomial regression extend the linear model, and what does it involve?", "answers": ["Polynomial regression extends the linear model by including additional predictor variables, obtained by raising the original predictors to various powers. For instance, a cubic regression includes variables X, X squared, and X cube. This approach provides a way to model non-linear relationships between predictors and the response."]}
{"id": 170, "contributed_by": "group 2", "question": "What is the purpose of step functions in regression modeling, and how do they affect the model's output?", "answers": ["Step functions in regression modeling are used to partition the range of a variable into distinct regions, effectively transforming it into a qualitative variable. This results in a piecewise constant function, allowing the model to capture abrupt changes in the relationship between predictors and the response."]}
{"id": 171, "contributed_by": "group 2", "question": "How do regression splines differ from polynomial regression, and what advantage do they offer in modeling non-linear relationships?", "answers": ["Regression splines are more flexible than polynomial regression and involve dividing the predictor range into distinct regions. Within each region, a polynomial function is fitted to the data, but these polynomials are constrained to join smoothly at region boundaries or knots. This approach offers greater flexibility and can model non-linear relationships without the excessive flexibility seen in high-degree polynomial regression."]}
{"id": 172, "contributed_by": "group 2", "question": "Why is it unusual to use a high degree of polynomial terms in polynomial regression, and what issues can arise with overly flexible polynomial curves?", "answers": ["It is unusual to use a high degree of polynomial terms in polynomial regression because as d increases, the polynomial curve becomes overly flexible. This flexibility can lead to overfitting, where the model captures noise in the data rather than true relationships. Additionally, near the boundary of predictor variables, high-degree polynomials can produce erratic and undesirable shapes, making interpretation and generalization challenging."]}
{"id": 173, "contributed_by": "group 2", "question": "What is the key strength of generalized additive models (GAMs) in comparison to linear models when it comes to multivariate regression?", "answers": ["The primary strength of generalized additive models (GAMs) lies in their ability to fit multivariate regression models with more flexibility than linear models."]}
{"id": 174, "contributed_by": "group 2", "question": "How is a GAM model constructed by hand for predicting wage, and which predictor variables are involved in this manual approach?", "answers": ["In the manual construction of a GAM for predicting wage, natural spline functions of year and age are used as predictor variables. Education is treated as a qualitative predictor in this model."]}
{"id": 175, "contributed_by": "group 2", "question": "Why is it necessary to build the model matrix manually when fitting a GAM, and what is the purpose of doing so in the context of constructing partial dependence plots?", "answers": ["Building the model matrix manually is important because it allows access to the individual components of the model. This manual approach is chosen to gain control and insight into the model's components when constructing partial dependence plots."]}
{"id": 176, "contributed_by": "group 2", "question": "What is the purpose of local regression, and how does it differ from other regression methods?", "answers": ["Local regression is a different approach for fitting flexible non-linear functions that compute the fit at a target point using nearby training observations. It differs from other regression methods in that it focuses on local data points to estimate the function."]}
{"id": 177, "contributed_by": "group 2", "question": "How does local regression use weights in the modeling process, and why are these weights unique for each value of x0?", "answers": ["Local regression uses weights (Ki0) to fit a new weighted least squares regression model at each point (x0). These weights are unique for each x0 because they are based on the nearby data points, and they need to be recalculated for each new point."]}
{"id": 178, "contributed_by": "group 2", "question": "Why is local regression sometimes referred to as a memory-based procedure, and what does it have in common with nearest-neighbors?", "answers": ["Local regression is called a memory-based procedure because, like nearest-neighbors, it requires all the training data each time it computes a prediction. Both methods rely on the entire dataset for predictions."]}
{"id": 179, "contributed_by": "group 2", "question": "What is the most crucial choice to make when performing local regression, and how does the 'span' parameter affect the process?", "answers": ["The most important choice in local regression is the 'span' (s), which determines the proportion of points used to compute the local regression at a specific point (x0). The span is similar to a tuning parameter and significantly influences the outcome of the local regression."]}
{"id": 180, "contributed_by": "group 2", "question": "When fitting a spline, what determines the flexibility of a regression spline, and how does knot placement influence this flexibility?", "answers": ["The flexibility of a regression spline is determined by the number and placement of knots. In regions with more knots, the polynomial coefficients can change rapidly, making the spline more flexible. Therefore, knot placement plays a crucial role in controlling the flexibility of the spline. Placing more knots in regions where rapid function changes are expected and fewer knots in stable regions is one option."]}
{"id": 181, "contributed_by": "group 2", "question": "What is a common practice in placing knots when fitting splines, and how is it achieved?", "answers": ["In practice, it is common to place knots in a uniform fashion. One approach to achieve this is to specify the desired degrees of freedom for the spline. Then, the software automatically places the corresponding number of knots at uniform quantiles of the data. This uniform knot placement simplifies the modeling process."]}
{"id": 182, "contributed_by": "group 2", "question": "How does specifying degrees of freedom and uniform knot placement affect spline modeling?", "answers": ["Specifying degrees of freedom and using uniform knot placement simplifies spline modeling. It allows for a more straightforward approach to control the number of knots and their placement. By specifying degrees of freedom, you can regulate the level of flexibility, making it a practical choice for many applications."]}
{"id": 183, "contributed_by": "group 2", "question": "What is a smoothing spline?", "answers": ["Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty"]}
{"id": 184, "contributed_by": "group 2", "question": "How is the penalty term in a smoothing spline defined?", "answers": ["The penalty term in a smoothing spline is defined as the integral of the square of the second derivative of the function."]}
{"id": 185, "contributed_by": "group 2", "question": "What does the tuning parameter in a smoothing spline control?", "answers": ["The tuning parameter in a smoothing spline controls the smoothness of the smoothing spline and, in turn, its effective degrees of freedom."]}
{"id": 186, "contributed_by": "group 2", "question": "How does the smoothing spline change as the tuning parameter increases?", "answers": ["As the tuning parameter increases from 0 to a larger value, the smoothing spline becomes smoother and more constrained."]}
{"id": 187, "contributed_by": "group 2", "question": "What are the unique properties of the function that minimizes the smoothing spline penalty?", "answers": ["The function is a piecewise cubic polynomial with continuous first and second derivatives at each knot, and it is linear outside of the extreme knots."]}
{"id": 188, "contributed_by": "group 2", "question": "In the context of smoothing splines, what is the significance of effective degrees of freedom?", "answers": ["Effective degrees of freedom indicate the flexibility of the smoothing spline and how heavily the parameters are constrained or shrunk."]}
{"id": 189, "contributed_by": "group 2", "question": "How is a smoothing spline different from a natural cubic spline?", "answers": ["A smoothing spline is a shrunken version of a natural cubic spline, and the tuning parameter controls the level of shrinkage."]}
{"id": 190, "contributed_by": "group 2", "question": "What happens when the tuning parameter is set to 0 in a smoothing spline?", "answers": ["When the tuning parameter is set to 0, the smoothing spline overfits the data and is very jumpy, exactly interpolating the training observations."]}
{"id": 191, "contributed_by": "group 2", "question": "What is the loss function in the smoothing spline formulation used for?", "answers": ["The loss function encourages the smoothing spline to fit the data well, minimizing the residual sum of squares."]}
{"id": 192, "contributed_by": "group 2", "question": "How does the smoothing spline approach the linear least squares line as the tuning parameter becomes large?", "answers": ["As the tuning parameter becomes large, the smoothing spline approaches the linear least squares line because it becomes perfectly smooth and linear."]}
{"id": 193, "contributed_by": "group 2", "question": "What are Generalized Additive Models (GAMs)?", "answers": ["Generalized Additive Models (GAMs) provide a framework for extending standard linear models by allowing non-linear functions of variables while maintaining additivity. They can be used for both quantitative and qualitative responses."]}
{"id": 194, "contributed_by": "group 2", "question": "What is the significance of the additivity in GAMs?", "answers": ["Additivity in GAMs means that we calculate a separate non-linear function for each predictor variable and then combine their contributions. This allows us to examine the effect of each predictor individually while holding others constant."]}
{"id": 195, "contributed_by": "group 2", "question": "How can non-linear relationships be modeled in GAMs?", "answers": ["In GAMs, non-linear relationships are modeled by replacing linear components with non-linear functions. These functions can be constructed using various methods, such as natural splines or smoothing splines."]}
{"id": 196, "contributed_by": "group 2", "question": "What is the advantage of using GAMs?", "answers": ["GAMs allow for the modeling of non-linear relationships, potentially resulting in more accurate predictions. They are flexible and can be applied to both quantitative and qualitative responses."]}
{"id": 197, "contributed_by": "group 2", "question": "What is one limitation of GAMs?", "answers": ["One limitation of GAMs is that they are restricted to being additive models. They may not capture complex interactions between predictor variables unless additional interaction terms are included in the model."]}
{"id": 198, "contributed_by": "group 2", "question": "How can interactions be included in a GAM?", "answers": ["To include interactions in a GAM, additional terms such as Xj x Xk can be added to the model. Alternatively, low-dimensional interaction functions like fjk(Xj, Xk) can be introduced and fitted using two-dimensional smoothers or splines."]}
{"id": 199, "contributed_by": "group 2", "question": "What type of models can be used as building blocks in GAMs?", "answers": ["Various models can be used as building blocks in GAMs, including natural splines, smoothing splines, local regression, and polynomial regression. GAMs provide a flexible framework that incorporates these models as needed."]}
{"id": 200, "contributed_by": "group 2", "question": "What is the method of comparing different polynomial degrees for model selection?", "answers": ["Different polynomial degrees can be compared for model selection using hypothesis tests. One can fit multiple polynomial models of varying degrees and perform ANOVA tests to determine the simplest model that adequately explains the relationship between the response and predictors."]}
